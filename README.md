# local-llm-fastapi
Local LLM backend service using Ollama (LLaMA 3.1) and FastAPI with structured JSON responses.
